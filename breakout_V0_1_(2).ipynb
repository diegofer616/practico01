{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegofer616/practico01/blob/main/breakout_V0_1_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install ale-py\n",
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNKek5QihiHG",
        "outputId": "a4c27e5b-d70d-460a-842d-355b56ba2029"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting ale-py\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py\n",
            "Successfully installed ale-py-0.10.1\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "igQC9jj8fXpS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras.backend as backend\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from tensorflow.python.framework.ops import enable_eager_execution\n",
        "\n",
        "from functools import partial\n",
        "print= partial(print, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A4ZTunZbgOAa"
      },
      "outputs": [],
      "source": [
        "import ale_py\n",
        "gym.register_envs(ale_py)\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ho7pNoFhCBa",
        "outputId": "7ac6dcd5-9484-45a4-e729-1f789bc6171a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set parameters\n"
          ]
        }
      ],
      "source": [
        "print('Set parameters')\n",
        "DEBUG = False\n",
        "SHOW_EVERY = 200\n",
        "\n",
        "DISCOUNT = 0.99                 # Discount factor gamma used in the Q-learning update\n",
        "LEARNING_RATE = 0.00025         # 0.00001   0.00025 Mnih et all 2015\n",
        "                                  # Hessel et al. 2017 used 0.0000625\n",
        "                                # in Pong use 0.00025\n",
        "REPLAY_MEMORY_SIZE = 1000000      #100000  # How many last steps to keep in memory for model training\n",
        "REPLAY_MEMORY_SIZE = 100000      #100000  # How many last steps to keep in memory for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = 50000   #50000  # Minimum number of random steps before to start training with the memory\n",
        "MIN_REPLAY_MEMORY_SIZE = 5000   #50000  # Minimum number of random steps before to start training with the memory\n",
        "                                # This is also the Number of completely random actions before the agent starts learning\n",
        "MAX_FRAMES = 25000000           #50milion # Total number of frames the agent sees during training\n",
        "MAX_FRAMES = 25000000           # Total number of frames the agent sees during training\n",
        "MINIBATCH_SIZE = 32             # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_MODEL =10000       #10000 # Number of chosen actions between updating the target network.\n",
        "UPDATE_TARGET_MODEL =100       #10000 # Number of chosen actions between updating the target network.\n",
        "\n",
        "                                       # According to Mnih et al. 2015 this is measured in the number of\n",
        "                                       # parameter updates (every four actions), however, in the\n",
        "                                       # DeepMind code, it is clearly measured in the number\n",
        "                                       # of actions the agent choses\n",
        "UPDATE_MODEL = 4\n",
        "\n",
        "MODEL_NAME = 'DQN_DeepMind'\n",
        "MIN_REWARD = -200  # For model save\n",
        "\n",
        "#EPISODES = 20000   # number of match played in total during training\n",
        "EPISODES = 500   # number of match played in total during training\n",
        "\n",
        "SAVE_EPISODE_EVERY= int(EPISODES/10)#save model 10 times in the whole run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eoCvtAi8hFeY"
      },
      "outputs": [],
      "source": [
        "# Exploration annealing settings\n",
        "epsilon = 1  # not a constant, going to be decayed\n",
        "MIN_EPSILON = 0.01\n",
        "MAX_EPSILON = 1.00\n",
        "MID_EPSILON = 0.1\n",
        "\n",
        "EPSILON_DECAY_1 = (MAX_EPSILON-MID_EPSILON)/(REPLAY_MEMORY_SIZE-MIN_REPLAY_MEMORY_SIZE)\n",
        "EPSILON_DECAY_2 = (MID_EPSILON-MIN_EPSILON)/(MAX_FRAMES-REPLAY_MEMORY_SIZE)\n",
        "\n",
        "#  Stats settings\n",
        "AGGREGATE_STATS_EVERY = 10  # episodes/match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fYakBJtBhZcQ"
      },
      "outputs": [],
      "source": [
        "class DQNAgent():\n",
        "    def __init__(self):\n",
        "        # Main model\n",
        "        self.model = self.create_model()\n",
        "        print(self.model.summary())\n",
        "\n",
        "        # Target network\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # Used to count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, (8, 8), input_shape=[84, 84, 4], strides=4,\n",
        "                         kernel_initializer=keras.initializers.VarianceScaling(scale=2.0)))  # 4 frame greyscale 84x84\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(64, (4, 4), strides=2, kernel_initializer=keras.initializers.VarianceScaling(scale=2.0)))\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), strides=1, kernel_initializer=keras.initializers.VarianceScaling(scale=2.0)))\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "        model.add(Dense(512, kernel_initializer=keras.initializers.VarianceScaling(scale=2.0), activation='relu'))\n",
        "\n",
        "        model.add(Dense(env.action_space.n, activation='linear',\n",
        "                        kernel_initializer=keras.initializers.VarianceScaling(scale=2.0)))  # action_space = how many choices (2)\n",
        "        #model.compile(loss=\"mse\", optimizer=Adam(lr=0.00025), metrics=['accuracy'])\n",
        "        model.compile(loss=tf.keras.losses.Huber(), optimizer=Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, minibatch):\n",
        "        # Get current states from minibatch, then query NN model for Q values\n",
        "        current_states = np.array(minibatch[0])/255\n",
        "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
        "\n",
        "        # Get future states from minibatch, then query NN model for Q values\n",
        "        new_current_states = np.array(minibatch[3])/255\n",
        "        future_qs_list = self.target_model.predict(new_current_states, verbose=0)\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        # Now we need to enumerate our batches\n",
        "        for ii in range(MINIBATCH_SIZE):\n",
        "            current_state = minibatch[0][ii]\n",
        "            action = minibatch[1][ii]\n",
        "            reward = minibatch[2][ii]\n",
        "            new_current_state = minibatch[3][ii]\n",
        "            done = minibatch[4][ii]\n",
        "\n",
        "            # Bellman equation.  Q = r + gamma*max Q',\n",
        "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
        "            # almost like with Q Learning, but we use just part of equation here\n",
        "            if not done:\n",
        "                new_q = reward + DISCOUNT * np.max(future_qs_list[ii])\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            # Update Q value for given state\n",
        "            current_qs = current_qs_list[ii]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # And append to our training data\n",
        "            X.append(current_state)\n",
        "            Y.append(current_qs)\n",
        "\n",
        "        # Fit on all samples as one batch, NO log file saved. Quicker simulation\n",
        "        # Note : not need to shuffle, this is already done in the get_minibatch part\n",
        "        self.model.fit(np.array(X), np.array(Y), batch_size=MINIBATCH_SIZE,\n",
        "                       verbose=0, epochs=1, shuffle=False)\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255, verbose=0)[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ylbKdrEZhbqQ"
      },
      "outputs": [],
      "source": [
        " # Trains main network every step during episode\n",
        "def train(self, minibatch):\n",
        "\n",
        "        # Get current states from minibatch, then query NN model for Q values\n",
        "        current_states = np.array(minibatch[0])/255\n",
        "        current_qs_list = self.model.predict(current_states,verbose=0)\n",
        "\n",
        "        # Get future states from minibatch, then query NN model for Q values\n",
        "        new_current_states = np.array(minibatch[3])/255\n",
        "        future_qs_list = self.target_model.predict(new_current_states,verbose=0)\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        # Now we need to enumerate our batches\n",
        "        for ii in range(MINIBATCH_SIZE):\n",
        "            current_state     = minibatch[0][ii]\n",
        "            action            = minibatch[1][ii]\n",
        "            reward            = minibatch[2][ii]\n",
        "            new_current_state = minibatch[3][ii]\n",
        "            done              = minibatch[4][ii]\n",
        "\n",
        "            # Bellman equation.  Q = r + gamma*max Q',\n",
        "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
        "            # almost like with Q Learning, but we use just part of equation here\n",
        "            if not done:\n",
        "                new_q = reward + DISCOUNT * np.max(future_qs_list[ii])\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            # Update Q value for given state\n",
        "            current_qs = current_qs_list[ii]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # And append to our training data\n",
        "            X.append(current_state)\n",
        "            Y.append(current_qs)\n",
        "\n",
        "        # Fit on all samples as one batch, NO log file saved. Quicker simulation\n",
        "        # Note : not need to shuffle, this is already done in the get_minibatch part\n",
        "        self.model.fit(np.array(X), np.array(Y), batch_size=MINIBATCH_SIZE,\n",
        "                       verbose=0, epochs=1,shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xJmaxzg1h2Xv"
      },
      "outputs": [],
      "source": [
        "# update target model function frames as verified into main (9*)\n",
        "def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.target_update_counter = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TuZrYyMjiAYI"
      },
      "outputs": [],
      "source": [
        "# Queries main network for Q values given current observation space (environment state)\n",
        "def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255,verbose=0)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lOgqOULvhLho"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(object):  # derived from https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\n",
        "    \"\"\"Replay Memory that stores the last \"size\" transitions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size=1000, frame_height=84, frame_width=84,\n",
        "                 agent_history_length=4, batch_size=32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            size: Integer, Number of stored transitions\n",
        "            frame_height: Integer, Height of a frame of an Atari game\n",
        "            frame_width: Integer, Width of a frame of an Atari game\n",
        "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
        "            batch_size: Integer, Number if transitions returned in a minibatch\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.frame_height = frame_height\n",
        "        self.frame_width = frame_width\n",
        "        self.agent_history_length = agent_history_length\n",
        "        self.batch_size = batch_size\n",
        "        self.count = 0\n",
        "        self.current = 0\n",
        "         # Pre-allocate memory\n",
        "        self.actions = np.empty(self.size, dtype=np.int32)\n",
        "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
        "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.terminal_flags = np.empty(self.size, dtype=bool)\n",
        "\n",
        "        # Pre-allocate memory for the states and new_states in a minibatch\n",
        "        self.states = np.empty((self.batch_size, self.agent_history_length,\n",
        "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.new_states = np.empty((self.batch_size, self.agent_history_length,\n",
        "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
        "\n",
        "    def add_experience(self, action, frame, reward, terminal):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action: An integer between 0 and env.action_space.n - 1\n",
        "                determining the action the agent performed  [a_t]\n",
        "            frame: A (84, 84, 1) frame of an Atari game in grayscale reached due to the action [s_t+1]\n",
        "            reward: A float determining the reward the agent received for performing an action [r_t]\n",
        "            terminal: A bool stating whether the episode terminated\n",
        "        \"\"\"\n",
        "        if frame.shape != (self.frame_height, self.frame_width):\n",
        "            raise ValueError('Dimension of frame is wrong!')\n",
        "        self.actions[self.current] = action\n",
        "        self.frames[self.current, ...] = frame\n",
        "        self.rewards[self.current] = reward\n",
        "        self.terminal_flags[self.current] = terminal\n",
        "        self.count = max(self.count, self.current + 1)\n",
        "        self.current = (self.current + 1) % self.size  # overwrite replay buffer if necessary\n",
        "\n",
        "    def _get_state(self, index):\n",
        "        if self.count == 0:\n",
        "            raise ValueError(\"The replay memory is empty!\")\n",
        "        if index < self.agent_history_length - 1:\n",
        "            raise ValueError(\"Index must be min 3\")\n",
        "        return self.frames[index - self.agent_history_length + 1:index + 1, ...]\n",
        "\n",
        "    def _get_valid_indices(self):\n",
        "        \"\"\"\n",
        "        We store all frames the agent sees in self.frames.\n",
        "        When a game terminates (terminal=True) at index i, frame at index i belongs\n",
        "        to a different episode than the frame at i+1. We want to avoid creating a state\n",
        "        with frames from two different episodes.\n",
        "        Finally we need to make sure that an index is not smaller than the number of\n",
        "        frames stacked together to create a state (self.agent_history_length=4),\n",
        "        so that a state and new_state can be sliced out of the array.\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            while True:\n",
        "                index = random.randint(self.agent_history_length, self.count - 1)\n",
        "                if index < self.agent_history_length:\n",
        "                    continue\n",
        "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
        "                    continue\n",
        "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
        "                    continue\n",
        "                break\n",
        "            self.indices[i] = index\n",
        "\n",
        "    def get_minibatch(self):\n",
        "        \"\"\"\n",
        "        Returns a minibatch of self.batch_size = 32 transitions\n",
        "        \"\"\"\n",
        "        if self.count < self.agent_history_length:\n",
        "            raise ValueError('Not enough memories to get a minibatch')\n",
        "\n",
        "        self._get_valid_indices()\n",
        "\n",
        "        for i, idx in enumerate(self.indices):\n",
        "            self.states[i] = self._get_state(idx - 1)\n",
        "            self.new_states[i] = self._get_state(idx)\n",
        "\n",
        "        minibatch = (np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[\n",
        "                     self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices])\n",
        "        return minibatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iC_YIupDjwRI"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Pre-processing utility\"\"\"\n",
        "\n",
        "def pre_processing (frame):\n",
        "    # single Frame Processor from 210x160x3 to 84x84x1\n",
        "    frame_gray = np.dot(frame, [0.299, 0.587, 0.114])  # 210x160  convert gray scale\n",
        "    #plt.imshow(np.array(np.squeeze(frame_gray)), cmap='gray')\n",
        "    #plt.show()\n",
        "    frame_gray = frame_gray[31:195, 0:160]  # crop off upper score (31 lines) and below black area (15 lines)\n",
        "    #resized_img0 = Image.fromarray(frame_gray).resize(size=(84, 84), resample=Image.BILINEAR)  # 84x84x1\n",
        "    resized_img0 = Image.fromarray(frame_gray).resize(size=(84, 84), resample=Image.NEAREST)  # 84x84x1\n",
        "    #plt.imshow(np.array(np.squeeze(resized_img0)), cmap='gray')\n",
        "    #plt.show()\n",
        "    return asarray(resized_img0, dtype=np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oxe2L5K4kb7o"
      },
      "outputs": [],
      "source": [
        "# Reset environment and get initial state\n",
        "reset_output = env.reset()\n",
        "\n",
        "# Handle different possible outputs of env.reset()\n",
        "if isinstance(reset_output, tuple):\n",
        "    current_frame = reset_output[0]\n",
        "else:\n",
        "    current_frame = reset_output\n",
        "\n",
        "# Preprocess the current frame\n",
        "current_frame = pre_processing(current_frame)\n",
        "current_state = np.dstack((current_frame, current_frame, current_frame, current_frame))  # create imm 84x84 grouping in 4 frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6M-8mygSlBjQ",
        "outputId": "86c168d8-7db7-419b-d353-a02a9631b160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main training loop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m8,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_18 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m32,832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_19 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_20 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,606,144\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │           \u001b[38;5;34m2,052\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,144</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,686,180\u001b[0m (6.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,180</span> (6.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,686,180\u001b[0m (6.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,180</span> (6.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?episodes/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main training loop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m8,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_24 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m32,832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_25 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_26 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,606,144\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │           \u001b[38;5;34m2,052\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,144</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,686,180\u001b[0m (6.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,180</span> (6.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,686,180\u001b[0m (6.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,180</span> (6.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/500 [00:00<?, ?episodes/s]\u001b[A\n",
            "  0%|          | 1/500 [00:00<04:20,  1.92episodes/s]\u001b[A\n",
            "  0%|          | 2/500 [00:00<03:04,  2.70episodes/s]\u001b[A\n",
            "  1%|          | 3/500 [00:01<03:44,  2.21episodes/s]\u001b[A\n",
            "  1%|          | 4/500 [00:01<04:22,  1.89episodes/s]\u001b[A\n",
            "  1%|1         | 5/500 [00:02<05:01,  1.64episodes/s]\u001b[A\n",
            "  1%|1         | 6/500 [00:03<04:56,  1.67episodes/s]\u001b[A\n",
            "  1%|1         | 7/500 [00:03<04:39,  1.76episodes/s]\u001b[A\n",
            "  2%|1         | 8/500 [00:04<04:54,  1.67episodes/s]\u001b[A\n",
            "  2%|1         | 9/500 [00:04<04:31,  1.81episodes/s]\u001b[A\n",
            "  2%|2         | 10/500 [00:05<04:00,  2.04episodes/s]\u001b[A\n",
            "  2%|2         | 11/500 [00:05<03:28,  2.35episodes/s]\u001b[A\n",
            "  2%|2         | 12/500 [00:06<03:31,  2.30episodes/s]\u001b[A\n",
            "  3%|2         | 13/500 [00:06<03:32,  2.29episodes/s]\u001b[A\n",
            "  3%|2         | 14/500 [00:06<03:17,  2.46episodes/s]\u001b[A\n",
            "  3%|3         | 15/500 [00:07<03:13,  2.51episodes/s]\u001b[A\n",
            "  3%|3         | 16/500 [00:07<03:10,  2.54episodes/s]\u001b[A\n",
            "  3%|3         | 17/500 [00:07<02:53,  2.78episodes/s]\u001b[A\n",
            "  4%|3         | 18/500 [00:08<03:07,  2.57episodes/s]\u001b[A\n",
            "  4%|3         | 19/500 [00:08<03:00,  2.67episodes/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:   20, frame_number:   3602, avg rew: 1.0, max rew: 2.0, min rew: 0.0, current epsilon:0.97142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  4%|4         | 20/500 [00:08<02:50,  2.81episodes/s]\u001b[A\n",
            "  4%|4         | 21/500 [00:09<02:52,  2.78episodes/s]\u001b[A\n",
            "  4%|4         | 22/500 [00:09<02:46,  2.87episodes/s]\u001b[A\n",
            "  5%|4         | 23/500 [00:09<02:38,  3.01episodes/s]\u001b[A\n",
            "  5%|4         | 24/500 [00:10<03:13,  2.46episodes/s]\u001b[A\n",
            "  5%|5         | 25/500 [00:10<03:20,  2.37episodes/s]\u001b[A\n",
            "  5%|5         | 26/500 [00:11<03:32,  2.23episodes/s]\u001b[A\n",
            "  5%|5         | 27/500 [00:11<03:07,  2.53episodes/s]\u001b[A\n",
            "  6%|5         | 28/500 [00:26<37:23,  4.75s/episodes]\u001b[A\n",
            "  6%|5         | 29/500 [00:45<1:10:05,  8.93s/episodes]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:   30, frame_number:   5586, avg rew: 1.6, max rew: 4.0, min rew: 0.0, current epsilon:0.99646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  6%|6         | 30/500 [01:02<1:29:20, 11.41s/episodes]\u001b[A\n",
            "  6%|6         | 31/500 [01:20<1:43:35, 13.25s/episodes]\u001b[A\n",
            "  6%|6         | 32/500 [01:41<2:01:54, 15.63s/episodes]\u001b[A\n",
            "  7%|6         | 33/500 [01:56<1:59:33, 15.36s/episodes]\u001b[A\n",
            "  7%|6         | 34/500 [02:07<1:50:26, 14.22s/episodes]\u001b[A\n",
            "  7%|7         | 35/500 [02:31<2:13:32, 17.23s/episodes]\u001b[A\n",
            "  7%|7         | 36/500 [02:50<2:15:48, 17.56s/episodes]\u001b[A\n",
            "  7%|7         | 37/500 [03:06<2:13:45, 17.33s/episodes]\u001b[A\n",
            "  8%|7         | 38/500 [03:17<1:57:56, 15.32s/episodes]\u001b[A\n",
            "  8%|7         | 39/500 [03:37<2:07:30, 16.60s/episodes]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:   40, frame_number:   7618, avg rew: 1.6, max rew: 3.0, min rew: 0.0, current epsilon:0.97737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  8%|8         | 40/500 [03:57<2:15:53, 17.73s/episodes]\u001b[A\n",
            "  8%|8         | 41/500 [04:14<2:14:12, 17.54s/episodes]\u001b[A\n",
            "  8%|8         | 42/500 [04:26<2:00:29, 15.78s/episodes]\u001b[A\n",
            "  9%|8         | 43/500 [04:42<2:01:48, 15.99s/episodes]\u001b[A\n",
            "  9%|8         | 44/500 [05:04<2:14:59, 17.76s/episodes]\u001b[A\n",
            "  9%|9         | 45/500 [05:18<2:06:02, 16.62s/episodes]\u001b[A\n",
            "  9%|9         | 46/500 [05:31<1:57:43, 15.56s/episodes]\u001b[A\n",
            "  9%|9         | 47/500 [05:42<1:47:15, 14.21s/episodes]\u001b[A\n",
            " 10%|9         | 48/500 [06:00<1:54:19, 15.18s/episodes]\u001b[A\n",
            " 10%|9         | 49/500 [06:18<2:01:35, 16.18s/episodes]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:   50, frame_number:   9347, avg rew: 1.0, max rew: 3.0, min rew: 0.0, current epsilon:0.96042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 10%|#         | 50/500 [06:34<2:00:24, 16.06s/episodes]\u001b[A\n",
            " 10%|#         | 51/500 [06:52<2:05:11, 16.73s/episodes]\u001b[A\n",
            " 10%|#         | 52/500 [07:04<1:54:09, 15.29s/episodes]\u001b[A\n",
            " 11%|#         | 53/500 [07:20<1:54:25, 15.36s/episodes]\u001b[A\n",
            " 11%|#         | 54/500 [07:42<2:10:12, 17.52s/episodes]\u001b[A\n",
            " 11%|#1        | 55/500 [07:56<2:02:11, 16.47s/episodes]\u001b[A\n",
            " 11%|#1        | 56/500 [08:25<2:29:05, 20.15s/episodes]\u001b[A\n",
            " 11%|#1        | 57/500 [08:41<2:20:25, 19.02s/episodes]\u001b[A\n",
            " 12%|#1        | 58/500 [08:58<2:13:53, 18.17s/episodes]\u001b[A\n",
            " 12%|#1        | 59/500 [09:12<2:06:09, 17.16s/episodes]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:   60, frame_number:  11289, avg rew: 1.2, max rew: 4.0, min rew: 0.0, current epsilon:0.94262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 12%|#2        | 60/500 [09:33<2:13:18, 18.18s/episodes]\u001b[A\n",
            " 12%|#2        | 61/500 [09:45<1:58:50, 16.24s/episodes]\u001b[A\n",
            " 12%|#2        | 62/500 [10:17<2:34:02, 21.10s/episodes]\u001b[A\n",
            " 13%|#2        | 63/500 [10:32<2:19:39, 19.18s/episodes]\u001b[A\n",
            " 13%|#2        | 64/500 [10:44<2:04:07, 17.08s/episodes]\u001b[A\n",
            " 13%|#3        | 65/500 [10:59<1:59:11, 16.44s/episodes]\u001b[A\n",
            " 13%|#3        | 66/500 [11:25<2:20:47, 19.46s/episodes]\u001b[A\n",
            " 13%|#3        | 67/500 [11:47<2:25:16, 20.13s/episodes]\u001b[A\n",
            " 14%|#3        | 68/500 [12:01<2:12:08, 18.35s/episodes]\u001b[A\n",
            " 14%|#3        | 69/500 [12:16<2:04:26, 17.32s/episodes]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:   70, frame_number:  13211, avg rew: 1.3, max rew: 5.0, min rew: 0.0, current epsilon:0.92355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 14%|#4        | 70/500 [12:30<1:55:51, 16.17s/episodes]\u001b[A\n",
            " 14%|#4        | 71/500 [12:50<2:03:37, 17.29s/episodes]\u001b[A\n",
            " 14%|#4        | 72/500 [13:09<2:07:33, 17.88s/episodes]\u001b[A\n",
            " 15%|#4        | 73/500 [13:27<2:07:41, 17.94s/episodes]\u001b[A\n",
            " 15%|#4        | 74/500 [13:40<1:57:50, 16.60s/episodes]\u001b[A\n",
            " 15%|#5        | 75/500 [14:01<2:05:53, 17.77s/episodes]\u001b[A\n",
            " 15%|#5        | 76/500 [14:27<2:22:29, 20.16s/episodes]\u001b[A\n",
            " 15%|#5        | 77/500 [14:47<2:22:14, 20.18s/episodes]\u001b[A\n",
            " 16%|#5        | 78/500 [15:02<2:11:12, 18.65s/episodes]\u001b[A\n",
            " 16%|#5        | 79/500 [15:23<2:15:23, 19.30s/episodes]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:   80, frame_number:  15218, avg rew: 1.7, max rew: 4.0, min rew: 0.0, current epsilon:0.90449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 16%|#6        | 80/500 [15:35<1:59:27, 17.07s/episodes]\u001b[A\n",
            " 16%|#6        | 81/500 [15:46<1:47:38, 15.41s/episodes]\u001b[A\n",
            " 16%|#6        | 82/500 [15:58<1:40:07, 14.37s/episodes]\u001b[A\n",
            " 17%|#6        | 83/500 [16:19<1:52:16, 16.15s/episodes]\u001b[A\n",
            " 17%|#6        | 84/500 [16:36<1:55:30, 16.66s/episodes]\u001b[A\n",
            " 17%|#7        | 85/500 [16:58<2:06:34, 18.30s/episodes]\u001b[A\n",
            " 17%|#7        | 86/500 [17:17<2:06:49, 18.38s/episodes]\u001b[A\n",
            " 17%|#7        | 87/500 [17:30<1:54:26, 16.62s/episodes]\u001b[A\n",
            " 18%|#7        | 88/500 [17:41<1:44:12, 15.18s/episodes]\u001b[A\n",
            " 18%|#7        | 89/500 [18:02<1:54:08, 16.66s/episodes]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:   90, frame_number:  16901, avg rew: 0.9, max rew: 2.0, min rew: 0.0, current epsilon:0.88850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 18%|#8        | 90/500 [18:13<1:44:02, 15.23s/episodes]\u001b[A\n",
            " 18%|#8        | 91/500 [18:27<1:39:33, 14.61s/episodes]\u001b[A\n",
            " 18%|#8        | 92/500 [18:52<2:02:22, 18.00s/episodes]\u001b[A\n",
            " 19%|#8        | 93/500 [19:14<2:08:38, 18.96s/episodes]\u001b[A\n",
            " 19%|#8        | 94/500 [19:36<2:15:35, 20.04s/episodes]\u001b[A"
          ]
        }
      ],
      "source": [
        "\"\"\"## Main training loop\"\"\"\n",
        "\n",
        "aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []}\n",
        "print('main training loop')\n",
        "# Create models folder\n",
        "if not os.path.isdir('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "agent = DQNAgent()\n",
        "print(\"The environment has the following {} actions: {}\".format(env.action_space.n,\n",
        "                                                                env.unwrapped.get_action_meanings()))\n",
        "\n",
        "\n",
        "my_replay_memory = ReplayMemory(size=REPLAY_MEMORY_SIZE, batch_size=MINIBATCH_SIZE)   # (â˜…)\n",
        "frame_number = 0   # total number of step conducted from the first match till the last\n",
        "# Iterate over episodes\n",
        "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):  #1 episode = 1 match\n",
        "\n",
        "    # Restarting episode - reset episode reward and step number\n",
        "    episode_reward = 0\n",
        "    step = 1\n",
        "\n",
        "    # Reset environment and get initial state\n",
        "#    current_frame,_ = env.reset()   # single frame 84x84\n",
        "    #NEW VERSION : take care, the new format of the result is a tuple ...\n",
        "    current_frame,_ = env.reset()   # single frame 84x84\n",
        "\n",
        "    current_frame = pre_processing(current_frame)\n",
        "    current_state = np.dstack((current_frame, current_frame, current_frame, current_frame)) # create imm 84x84 grouping in 4 frames\n",
        "\n",
        "    # Reset flag and start iterating until episode ends\n",
        "    done = False\n",
        "    while not done:   #in the environment MountainCar-v0 done=False after 200 step\n",
        "                      #in the environment Breakout-v4 done=False after lost all lifes (num life =5)\n",
        "        # Exploration-exploitation trade-off\n",
        "        # Take new action\n",
        "        # train main network\n",
        "        # Set new state\n",
        "        # Add new reward\n",
        "\n",
        "        # Exploration-exploitation trade-off after a number of steps with completely random action\n",
        "        if np.random.random() > epsilon and frame_number > MIN_REPLAY_MEMORY_SIZE:\n",
        "            # Get action from DQN model: exploit\n",
        "            action = np.argmax(agent.get_qs(current_state))\n",
        "        else:\n",
        "            # Get random action: explore\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        # Take new action\n",
        "#        new_frame, reward, done, _ = env.step(action)\n",
        "        #new format\n",
        "        new_frame, reward, done1,done2, _ = env.step(action)\n",
        "        done = done1 |done2\n",
        "        reward = np.clip(reward,-1,1)#clip reward to be between -1,1\n",
        "        # Set and preprocess new state\n",
        "        new_frame = pre_processing(new_frame)  # single frame 84x84 preprocessed\n",
        "        new_state = np.dstack((new_frame, current_state[:, :, 0], current_state[:, :, 1], current_state[:, :, 2]))# create imm 84x84 grouping in 4 frames\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if episode % SHOW_EVERY == 0: # and DEBUG: # plot one match every SHOW_EVERY\n",
        "            #time.sleep(0.01)\n",
        "            #print(f'Step: {step}')\n",
        "            env.render()\n",
        "\n",
        "        # Every step/frame we update replay memory with action, new frame, reward due to the action\n",
        "        my_replay_memory.add_experience(action, new_frame, reward, done)\n",
        "\n",
        "        # Every step we evaluate to train main network and/or to update weights of target network\n",
        "        if frame_number % UPDATE_MODEL == 0 and frame_number > MIN_REPLAY_MEMORY_SIZE:  # model update every 4 frame/action\n",
        "            # Get a minibatch of random samples from the replay memory\n",
        "            minibatch = my_replay_memory.get_minibatch()\n",
        "            agent.train(minibatch)                  #\n",
        "            if DEBUG:  # plot of the minibatch used to train (only 1 image over 4 into the frame)\n",
        "                fig = plt.figure(figsize=(8*4, 4*4))\n",
        "                for i in range(my_replay_memory.batch_size):\n",
        "                    plt.subplot(4, 8, i + 1)\n",
        "                    plt.imshow(minibatch[0][i, :, :, 0], cmap='jet')\n",
        "                plt.show()\n",
        "                plt.savefig('minibatch'+str(frame_number)+'.jpg')\n",
        "            del minibatch\n",
        "        if frame_number % UPDATE_TARGET_MODEL == 0 and frame_number > MIN_REPLAY_MEMORY_SIZE: # model target update every 10000 frame/action\n",
        "            agent.update_target_model()    # (9â˜…)\n",
        "        #\n",
        "\n",
        "\n",
        "        # Set new state (9*)\n",
        "        current_state = new_state\n",
        "        step += 1\n",
        "        frame_number += 1\n",
        "\n",
        "                # Inicializa ep_rewards como una lista vacía\n",
        "        ep_rewards = []\n",
        "\n",
        "                # Main training loop\n",
        "        print('main training loop')\n",
        "        # Create models folder\n",
        "        if not os.path.isdir('models'):\n",
        "            os.makedirs('models')\n",
        "\n",
        "        agent = DQNAgent()\n",
        "        print(\"The environment has the following {} actions: {}\".format(env.action_space.n,\n",
        "                                                                        env.unwrapped.get_action_meanings()))\n",
        "\n",
        "        my_replay_memory = ReplayMemory(size=REPLAY_MEMORY_SIZE, batch_size=MINIBATCH_SIZE)   # (★)\n",
        "        frame_number = 0   # total number of step conducted from the first match till the last\n",
        "        # Inicializa ep_rewards como una lista vacía\n",
        "        ep_rewards = []\n",
        "\n",
        "        # Iterate over episodes\n",
        "        for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):  #1 episode = 1 match\n",
        "\n",
        "            # Restarting episode - reset episode reward and step number\n",
        "            episode_reward = 0\n",
        "            step = 1\n",
        "\n",
        "            # Reset environment and get initial state\n",
        "            reset_output = env.reset()\n",
        "\n",
        "            # Handle different possible outputs of env.reset()\n",
        "            if isinstance(reset_output, tuple):\n",
        "                current_frame = reset_output[0]\n",
        "            else:\n",
        "                current_frame = reset_output\n",
        "\n",
        "            # Preprocess the current frame\n",
        "            current_frame = pre_processing(current_frame)\n",
        "            current_state = np.dstack((current_frame, current_frame, current_frame, current_frame))  # create imm 84x84 grouping in 4 frames\n",
        "\n",
        "            # Reset flag and start iterating until episode ends\n",
        "            done = False\n",
        "            while not done:   #in the environment MountainCar-v0 done=False after 200 step\n",
        "                              #in the environment Breakout-v4 done=False after lost all lifes (num life =5)\n",
        "                # Exploration-exploitation trade-off\n",
        "                # Take new action\n",
        "                # train main network\n",
        "                # Set new state\n",
        "                # Add new reward\n",
        "\n",
        "                # Exploration-exploitation trade-off after a number of steps with completely random action\n",
        "                if np.random.random() > epsilon and frame_number > MIN_REPLAY_MEMORY_SIZE:\n",
        "                    # Get action from DQN model: exploit\n",
        "                    action = np.argmax(agent.get_qs(current_state))\n",
        "                else:\n",
        "                    # Get random action: explore\n",
        "                    action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "                # Take new action\n",
        "                new_frame, reward, done1, done2, _ = env.step(action)\n",
        "                done = done1 | done2\n",
        "                reward = np.clip(reward, -1, 1)  # clip reward to be between -1,1\n",
        "                # Set and preprocess new state\n",
        "                new_frame = pre_processing(new_frame)  # single frame 84x84 preprocessed\n",
        "                new_state = np.dstack((new_frame, current_state[:, :, 0], current_state[:, :, 1], current_state[:, :, 2]))  # create imm 84x84 grouping in 4 frames\n",
        "\n",
        "                episode_reward += reward\n",
        "\n",
        "                if episode % SHOW_EVERY == 0:  # and DEBUG: # plot one match every SHOW_EVERY\n",
        "                    env.render()\n",
        "\n",
        "                # Every step/frame we update replay memory with action, new frame, reward due to the action\n",
        "                my_replay_memory.add_experience(action, new_frame, reward, done)\n",
        "\n",
        "                # Every step we evaluate to train main network and/or to update weights of target network\n",
        "                if frame_number % UPDATE_MODEL == 0 and frame_number > MIN_REPLAY_MEMORY_SIZE:  # model update every 4 frame/action\n",
        "                    # Get a minibatch of random samples from the replay memory\n",
        "                    minibatch = my_replay_memory.get_minibatch()\n",
        "                    agent.train(minibatch)\n",
        "                    if DEBUG:  # plot of the minibatch used to train (only 1 image over 4 into the frame)\n",
        "                        fig = plt.figure(figsize=(8*4, 4*4))\n",
        "                        for i in range(my_replay_memory.batch_size):\n",
        "                            plt.subplot(4, 8, i + 1)\n",
        "                            plt.imshow(minibatch[0][i, :, :, 0], cmap='jet')\n",
        "                        plt.show()\n",
        "                        plt.savefig('minibatch'+str(frame_number)+'.jpg')\n",
        "                    del minibatch\n",
        "                if frame_number % UPDATE_TARGET_MODEL == 0 and frame_number > MIN_REPLAY_MEMORY_SIZE:  # model target update every 10000 frame/action\n",
        "                    agent.update_target_model()\n",
        "\n",
        "                # Set new state\n",
        "                current_state = new_state\n",
        "                step += 1\n",
        "                frame_number += 1\n",
        "\n",
        "                        # Append episode reward to a list and log stats (every given number of episodes)\n",
        "            ep_rewards.append(episode_reward)\n",
        "            if episode % 10 == 0 and episode > AGGREGATE_STATS_EVERY:\n",
        "                average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "                min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "                max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "                aggr_ep_rewards['ep'].append(episode)\n",
        "                aggr_ep_rewards['avg'].append(average_reward)\n",
        "                aggr_ep_rewards['max'].append(max_reward)\n",
        "                aggr_ep_rewards['min'].append(min_reward)\n",
        "                print(f'Episode:{episode:>5d}, frame_number:{frame_number:>7d}, ' +\n",
        "                      f'avg rew:{average_reward:>4.1f}, ' +\n",
        "                      f'max rew:{max(ep_rewards[-AGGREGATE_STATS_EVERY:]):>4.1f}, ' +\n",
        "                      f'min rew:{min(ep_rewards[-AGGREGATE_STATS_EVERY:]):>4.1f}, ' +\n",
        "                      f'current epsilon:{epsilon:>1.5f}')\n",
        "\n",
        "            # Decay epsilon. Only start after replay memory is over min size\n",
        "            if frame_number > MIN_REPLAY_MEMORY_SIZE:\n",
        "                if frame_number < REPLAY_MEMORY_SIZE:\n",
        "                    epsilon = MID_EPSILON + EPSILON_DECAY_1*(REPLAY_MEMORY_SIZE-frame_number)\n",
        "                else:\n",
        "                    epsilon = MIN_EPSILON + EPSILON_DECAY_2*(MAX_FRAMES-frame_number)\n",
        "            epsilon = np.clip(epsilon, MIN_EPSILON, MAX_EPSILON)\n",
        "\n",
        "            if episode % SAVE_EPISODE_EVERY == 0 and episode > AGGREGATE_STATS_EVERY:\n",
        "                agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}' +\n",
        "                                 f'max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}' +\n",
        "                                 f'min__{int(time.time())}.keras')\n",
        "\n",
        "        env.close()\n",
        "        # Save model\n",
        "        agent.model.save(f'models/{MODEL_NAME}__' +\n",
        "                         f'{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_' +\n",
        "                         f'{min_reward:_>7.2f}min__{int(time.time())}.keras')\n",
        "\n",
        "        plt.figure('stats')\n",
        "        plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label=\"average rewards\")\n",
        "        plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label=\"max rewards\")\n",
        "        plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label=\"min rewards\")\n",
        "        plt.legend(loc=2)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        plt.savefig('grid.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tTUI79P6lSBn",
        "outputId": "163a4690-e69e-4b99-bba8-54e281d12a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "debug and test results\n",
            "play game and draw results\n",
            "best action= NOOP frame= 1\n",
            "best action= NOOP frame= 2\n",
            "best action= NOOP frame= 3\n",
            "best action= NOOP frame= 4\n",
            "best action= NOOP frame= 5\n",
            "best action= NOOP frame= 6\n",
            "best action= NOOP frame= 7\n",
            "best action= NOOP frame= 8\n",
            "best action= NOOP frame= 9\n",
            "best action= NOOP frame= 10\n",
            "best action= NOOP frame= 11\n",
            "best action= NOOP frame= 12\n",
            "best action= NOOP frame= 13\n",
            "best action= NOOP frame= 14\n",
            "best action= NOOP frame= 15\n",
            "best action= NOOP frame= 16\n",
            "best action= NOOP frame= 17\n",
            "best action= NOOP frame= 18\n",
            "best action= NOOP frame= 19\n",
            "best action= NOOP frame= 20\n",
            "make animation\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/gif": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Plot some mini-batch at random\n",
        "\n",
        "print('debug and test results')\n",
        "if(False):\n",
        "    print('plot some mini-batch at random')\n",
        "    minibatch = my_replay_memory.get_minibatch()\n",
        "    no_frames = my_replay_memory.agent_history_length\n",
        "    fig = plt.figure(figsize=(MINIBATCH_SIZE*4, no_frames*4))\n",
        "    for ii in range(no_frames):\n",
        "        for iii in range(MINIBATCH_SIZE):\n",
        "            plt.subplot(my_replay_memory.agent_history_length, MINIBATCH_SIZE, iii+ii*MINIBATCH_SIZE+1)\n",
        "            plt.imshow(minibatch[0][iii, :, :, ii], cmap='jet')  # images are in minibatch[0] stored as minibatch[0].shape=(32, 84, 84, 4) ndarray\n",
        "            plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "env_test = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
        "action_meanings = env_test.unwrapped.get_action_meanings()\n",
        "\n",
        "if(False):\n",
        "    print('load model from directory model_bak')\n",
        "    loaded_model = keras.models.load_model('model_bak')\n",
        "    agent.model.set_weights(loaded_model.get_weights())\n",
        "\n",
        "print('play game and draw results')\n",
        "import matplotlib.animation as animation\n",
        "images = []  # List to store the generated images\n",
        "\n",
        "current_frame, _ = env_test.reset()  # single frame 84x84\n",
        "current_frame_pre = pre_processing(current_frame)\n",
        "# initialize\n",
        "current_states = np.zeros(current_frame_pre.shape + (4,))\n",
        "current_states[:, :, 0] = current_frame_pre.copy()\n",
        "\n",
        "for jj in range(3):\n",
        "    new_frame, reward, done1, done2, _ = env_test.step(action)\n",
        "    current_states[:, :, jj+1] = pre_processing(new_frame)\n",
        "\n",
        "plt.figure('play')\n",
        "done = False\n",
        "count = 0\n",
        "max_frames_play = 20\n",
        "while (not done) and (count < max_frames_play):\n",
        "    count += 1\n",
        "    current_qs_list = agent.model.predict(current_states[None, ...], verbose=0)  # input_shape=[1,84, 84, 4]\n",
        "    best_action = np.argmax(current_qs_list)\n",
        "    print('best action=', action_meanings[best_action], 'frame=', str(count))\n",
        "    new_current_states = current_states.copy()\n",
        "    new_current_states[:, :, :-1] = current_states[:, :, 1:]  # shift old states down a unit, put new one after\n",
        "    new_frame, reward, done1, done2, _ = env_test.step(best_action)\n",
        "    new_current_states[:, :, -1] = pre_processing(new_frame)\n",
        "    done = done1 | done2\n",
        "    images.append(new_frame)\n",
        "    if(done):\n",
        "        break\n",
        "#  plt.imshow(new_frame)\n",
        "#  plt.savefig(\"play_\"+str(count)+\".jpg\")\n",
        "#  plt.pause(0.1)\n",
        "\n",
        "print('make animation')\n",
        "fig, ax = plt.subplots()\n",
        "# Function to update the plot for each frame of the animation\n",
        "def update(frame):\n",
        "    ax.imshow(images[frame])  # Display the corresponding image for the current frame\n",
        "\n",
        "ani = animation.FuncAnimation(fig, update, frames=len(images), interval=400)\n",
        "ani.save('animation.gif', writer='pillow')  # Requires pillow to be installed\n",
        "plt.show()\n",
        "\n",
        "from IPython.display import Image as Image_Ip\n",
        "from IPython.display import display\n",
        "display(Image_Ip(filename='animation.gif'))\n",
        "#if('ipykernel' in sys.modules):#we are in Jupyter Notebook\n",
        "#if('google.colab' in sys.modules):#we are in google colab"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}